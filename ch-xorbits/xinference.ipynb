{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-xinference)=\n",
    "# Xinference\n",
    "\n",
    "Xorbits Inference (Xinference) is an inference platform for large models, supporting large language models, vector models, and text-to-image models. It is based on the distributed computation provided by [Xoscar](https://github.com/xorbitsai/xoscar), allowing models to be deployed on a cluster. The platform offers an OpenAI-like interface, enabling users to deploy and call open-source large models. Xinference integrates the API for external services, inference engine, and hardware, eliminating the need to write code to manage model inference services like Ray Serve.\n",
    "\n",
    "## Inference Engine\n",
    "\n",
    "Xinference can adapt to different inference engines, including Hugging Face Transformers, [vLLM](https://github.com/vllm-project/vllm), [llama.cpp](https://github.com/ggerganov/llama.cpp), etc. Therefore, you need to install the corresponding inference engine during installation, such as `pip install \"xinference[transformers]\"`. Transformers is entirely based on PyTorch, offering the fastest and most comprehensive model compatibility, but with poorer performance; other inference engines, such as vLLM and llama.cpp, focus on performance optimization but do not cover as many models as Transformers.\n",
    "\n",
    "## Cluster\n",
    "\n",
    "Before using, you need to start a Xinference cluster, which can be either single-machine multi-GPU or multi-machine multi-GPU. On a single machine, you can start it from the command line like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "xinference-local --host 0.0.0.0 --port 9997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster setup is similar to Xorbits Data. First, start a Supervisor, then start the Worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Start the Supervisor\n",
    "xinference-supervisor -H <supervisor_ip>\n",
    "\n",
    "# Start the Worker\n",
    "xinference-worker -e \"http://<supervisor_ip>:9997\" -H <worker_ip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, you can access the Xinference service at http://<supervisor_ip>:9997.\n",
    "\n",
    "## Using Models\n",
    "\n",
    "Xinference provides full lifecycle management for models, including starting, running, and shutting down models. Once the Xinference service is started, users can start and use models. Xinference supports various open-source models, allowing users to select and start models through a web interface. Xinference will automatically download and initialize the required models in the backend. Each model comes with a web-based conversation interface and provides an OpenAI API-compatible interface.\n",
    "\n",
    "Next, we will demonstrate how to use Xinference in a local environment through two examples, how to interact with Xinference using the OpenAI API, and how to build intelligent systems by using LangChain and vector database technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using Llama for Simple Text Generation and Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, in addition to installing Xinference, you also need to install the openai dependency package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xinference[transformers] openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start a local instance of Xinference. In a Jupyter Notebook, use the following command to run Xinference in the background. In the command line, you can directly use `xinference-local --host 0.0.0.0 --port 9997`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is not running, starting service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ps ax | grep -v grep | grep \"xinference-local\" > /dev/null\n",
    "then\n",
    "    echo \"Service is already running, exiting.\"\n",
    "else\n",
    "    echo \"Service is not running, starting service.\"\n",
    "    nohup xinference-local --host 0.0.0.0 --port 9997 > xinference.log 2>&1 &\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default host and IP address for Xinference are 127.0.0.1 and 9997, respectively.\n",
    "\n",
    "Next, use the following command to start the Llama model. The `--size-in-billion` parameter corresponds to the parameter scale used. The first-generation Llama model (code-named `llama-3-instruct` in Xinference) supports parameter scales of 8 billion, 70 billion, and 70 billion. The `--quantization` parameter specifies the precision reduction method (options: 4-bit, 8-bit, or none for full precision). Here we'll use the 8B model with 8-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch model name: llama-3-instruct with kwargs: {}\n",
      "Model uid: my-llm\n"
     ]
    }
   ],
   "source": [
    "!xinference launch \\\n",
    "  --model-uid my-llm \\\n",
    "  --model-name llama-3-instruct \\\n",
    "  --size-in-billions 8 \\\n",
    "  --quantization 8-bit \\\n",
    "  --model-format pytorch \\\n",
    "  --model-engine transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting the model for the first time, Xinference will automatically download the model, which may take some time.\n",
    "\n",
    "Since Xinference provides an OpenAI-compatible API, you can treat the model running on Xinference as a local alternative to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"can be empty\", base_url=\"http://127.0.0.1:9997/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the OpenAI API to easily use the large model for text generation and conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion API\n",
    "\n",
    "We can perform text generation using OpenAI's `client.completions.create` method. The Completion API is used to guide the model to generate text based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'detail': \"[address=0.0.0.0:40957, pid=52333] 'str' object has no attribute 'get'\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | top_p: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a short two or three-line poem about AI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m complete_and_print(prompt)\n",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m, in \u001b[0;36mcomplete_and_print\u001b[0;34m(prompt, temperature, top_p, client, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete_and_print\u001b[39m(\n\u001b[1;32m      2\u001b[0m     prompt, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, client\u001b[38;5;241m=\u001b[39mclient, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-llm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m ):\n\u001b[1;32m      4\u001b[0m     response \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 5\u001b[0m         client\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      6\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel, prompt\u001b[38;5;241m=\u001b[39mprompt, top_p\u001b[38;5;241m=\u001b[39mtop_p, temperature\u001b[38;5;241m=\u001b[39mtemperature\n\u001b[1;32m      7\u001b[0m         )\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[temperature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | top_p: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/resources/completions.py:539\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    538\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    541\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    542\u001b[0m             {\n\u001b[1;32m    543\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    544\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m    545\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_of,\n\u001b[1;32m    546\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecho\u001b[39m\u001b[38;5;124m\"\u001b[39m: echo,\n\u001b[1;32m    547\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    548\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    549\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    550\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    551\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    552\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    553\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    555\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    556\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    557\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: suffix,\n\u001b[1;32m    558\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    559\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    560\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    561\u001b[0m             },\n\u001b[1;32m    562\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    563\u001b[0m         ),\n\u001b[1;32m    564\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    565\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    566\u001b[0m         ),\n\u001b[1;32m    567\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mCompletion,\n\u001b[1;32m    568\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[Completion],\n\u001b[1;32m    570\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    961\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    962\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    963\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    964\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    965\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    966\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1050\u001b[0m         input_options,\n\u001b[1;32m   1051\u001b[0m         cast_to,\n\u001b[1;32m   1052\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1053\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1054\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1055\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1099\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1100\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1101\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1102\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1103\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1104\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1050\u001b[0m         input_options,\n\u001b[1;32m   1051\u001b[0m         cast_to,\n\u001b[1;32m   1052\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1053\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1054\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1055\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1099\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1100\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1101\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1102\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1103\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1104\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'detail': \"[address=0.0.0.0:40957, pid=52333] 'str' object has no attribute 'get'\"}"
     ]
    }
   ],
   "source": [
    "def complete_and_print(\n",
    "    prompt, temperature=0.7, top_p=0.9, client=client, model=\"my-llm\"\n",
    "):\n",
    "    response = (\n",
    "        client.completions.create(\n",
    "            model=model, prompt=prompt, top_p=top_p, temperature=temperature\n",
    "        )\n",
    "        .choices[0]\n",
    "        .text\n",
    "    )\n",
    "\n",
    "    print(f\"[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n\")\n",
    "\n",
    "\n",
    "prompt = \"Write a short two or three-line poem about AI.\"\n",
    "complete_and_print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust some parameters provided by the API to configure the creativity and determinism of the output.\n",
    "\n",
    "The `top_p` means the cumulative probability cutoff for token selection, which controls how many tokens to choose, while the `temperature` parameter determines whether there is randomness in text generation within this range. When the temperature is close to 0, the result will be almost deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.01 | top_p: 0.01]\n",
      "It can be serious, funny, ironic, whatever you want.\n",
      "I am not sure if this is what you are looking for but I wrote a poem about AI and the future of humanity.\n",
      "\n",
      "[temperature: 0.01 | top_p: 0.01]\n",
      "It can be serious, funny, ironic, whatever you want.\n",
      "I am not sure if this is what you are looking for but I wrote a poem about AI and the future of humanity.\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Post it (with your email address, just in case!) and then vote for which you love the best, and why. You have until August 5 to submit poems, and I will post results as soon as they're ready.\n",
      "I'm working on mine! Thanks for this prompt!\n",
      "And yes, there should be poetry from the robots themselves, when they take over!\n",
      "This will be great fun! But we need some way to collect votes...?\n",
      "We'll figure something out! Thanks for reading.\n",
      "My AI is still in incubation....But I like these lines of yours that caught my attention so deeply that my mind lingered over them long enough to make me think of the AI \"personality\"...it's quite a compliment!\n",
      "Very intriguing lines indeed! And perhaps my robot self could write more and maybe better ones some day?\n",
      "Ohh this sounds interesting, now I gotta go write something!\n",
      "Will definitely have to write one - but probably not till later, though.\n",
      "Wow, thank you both very much!\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "The lines can be one-word poems, long single lines, and they could be free-verse in any form you wish.\n",
      "For the 2/20/2019 prompt pick one or more of the two themes: \"art\"and \"mystery\"\n",
      "I do not live by bread alone.\n",
      "I am a stranger living in a strange land.\n",
      "and you will find me.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first two generations will be very similar,\n",
    "complete_and_print(prompt, temperature=0.01, top_p=0.01)\n",
    "complete_and_print(prompt, temperature=0.01, top_p=0.01)\n",
    "\n",
    "# The last two generations will be different\n",
    "complete_and_print(prompt, temperature=1.0, top_p=1.0)\n",
    "complete_and_print(prompt, temperature=1.0, top_p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion API\n",
    "Next, we will use `client.chat.completions.create` for contextual conversation.\n",
    "\n",
    "The Chat Completion API provides a more structured way to interact with large language models (LLMs). Instead of traditional text input, we send an array containing multiple structured information objects to the LLM as input. This input method allows the large language model to reference \"context\" or \"history\" when generating responses.\n",
    "\n",
    "Typically, each piece of information will have a `role` and `content`:\n",
    "\n",
    "- The `system` role is used to convey core instructions defined by the developer to the language model.\n",
    "- The `user` role represents the requests sent by the user to the language model.\n",
    "- The `assistant` role is the response returned by the language model to the user's request.\n",
    "\n",
    "First, we define the structured information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(content: str):\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "\n",
    "\n",
    "def user(content: str):\n",
    "    return {\"role\": \"user\", \"content\": content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using the Chat Completion API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "assistant: You told me earlier that your favorite color is BLUE!\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: You told me earlier that your pet's name is Lucy, which is a lovely name for a dog!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat_complete_and_print(\n",
    "    messages, temperature=0.7, top_p=0.9, client=client, model=\"my-llm\"\n",
    "):\n",
    "    response = (\n",
    "        client.chat.completions.create(\n",
    "            model=model, messages=messages, top_p=top_p, temperature=temperature\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    print(f\"==============\\nassistant: {response}\\n\\n\")\n",
    "\n",
    "\n",
    "chat_complete_and_print(\n",
    "    messages=[\n",
    "        user(\"My favorite color is blue\"),\n",
    "        assistant(\"That's wonderful to hear!\"),\n",
    "        user(\"What is my favorite color?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_complete_and_print(\n",
    "    messages=[\n",
    "        user(\"I have a little dog named Lucy\"),\n",
    "        assistant(\"That's awesome! Lucy must be very cute.\"),\n",
    "        user(\"What is my pet's name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can still adjust different `temperature` and `top_p` parameters to show how different settings affect the randomness and diversity of the generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "assistant: Learning to play the piano can have numerous benefits, including:\n",
      "\n",
      "* Improved cognitive skills: Playing piano requires coordination between hands, eyes, and brain, which can improve memory, concentration, and problem-solving abilities.\n",
      "* Enhanced creativity: Piano playing allows for self-expression and creativity through music composition and improvisation.\n",
      "* Stress relief: Playing piano can be a calming and meditative experience, reducing stress and anxiety.\n",
      "* Brain development: Research suggests that early childhood piano lessons can even affect brain structure and function, improving spatial-temporal skills and language development.\n",
      "* Social benefits: Playing piano can provide opportunities to connect with others through music-making, whether it's performing in front of an audience or jamming with friends.\n",
      "\n",
      "These are just a few examples, but I'm sure you're experiencing many more benefits as you learn and enjoy playing the piano!\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: Learning to play the piano can have numerous benefits, including:\n",
      "\n",
      "* Improved cognitive skills: Playing piano requires coordination between hands, eyes, and brain, which can improve memory, concentration, and problem-solving abilities.\n",
      "* Enhanced creativity: Piano playing allows for self-expression and creativity through music composition and improvisation.\n",
      "* Stress relief: Playing piano can be a calming and meditative experience, reducing stress and anxiety.\n",
      "* Brain development: Research suggests that early childhood piano lessons can even affect brain structure and function, improving spatial-temporal skills and language development.\n",
      "* Social benefits: Playing piano can provide opportunities to connect with others through music-making, whether it's performing in front of an audience or jamming with friends.\n",
      "\n",
      "These are just a few examples, but I'm sure you're experiencing many more benefits as you learn and enjoy playing the piano!\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: Learning to play the piano can bring many benefits, including:\n",
      "\n",
      "* Improved cognitive skills: playing the piano requires coordination between hands and brain, which can improve memory, concentration, and spatial-temporal skills.\n",
      "* Enhanced creativity: composing and improvising music can foster creative thinking and self-expression.\n",
      "* Emotional intelligence: playing emotional music can help develop empathy and understanding of others' emotions.\n",
      "* Language development: research suggests that musical training can improve language skills and even delay symptoms of Alzheimer's disease.\n",
      "* Stress relief: playing the piano can be a calming and meditative experience, reducing anxiety and stress levels.\n",
      "\n",
      "These are just a few examples, but I'm sure you're experiencing them firsthand as you learn to play the piano! Keep practicing and enjoying it!\n",
      "\n",
      "\n",
      "==============\n",
      "assistant: Learning to play the piano can have many benefits, including:\n",
      "\n",
      "* Improved cognitive skills: Playing the piano requires coordination between different parts of the brain, which can improve memory, concentration, and spatial-temporal skills.\n",
      "* Enhanced creativity and self-expression\n",
      "* Boosted confidence and self-esteem through mastery of new skills\n",
      "* Stress relief and relaxation through playing calming music or meditative pieces\n",
      "* Language development for children (piano lessons can even be beneficial for language learning)\n",
      "* Social benefits from sharing your love of music with others, whether by performing or teaching\n",
      "* Brain plasticity and adaptability, as your brain reorganizes itself in response to new musical demands.\n",
      "\n",
      "These benefits can translate to other areas of life, such as personal relationships, work, and overall well-being!\n",
      "\n",
      "How about you, what has been most enjoyable or surprising about learning piano so far?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    user(\"I've been learning piano recently.\"),\n",
    "    assistant(\"That's really a great hobby!\"),\n",
    "    user(\"What do you think are the benefits of learning this instrument? Tell me briefly\"),\n",
    "]\n",
    "\n",
    "\n",
    "# More deterministic results\n",
    "chat_complete_and_print(messages, temperature=0.1, top_p=0.1)\n",
    "chat_complete_and_print(messages, temperature=0.1, top_p=0.1)\n",
    "\n",
    "# More random results\n",
    "chat_complete_and_print(messages, temperature=1.0, top_p=1.0)\n",
    "chat_complete_and_print(messages, temperature=1.0, top_p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the inference service is no longer needed, you can shut down the background running Xinference instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps ax | grep xinference-local | grep -v grep | awk '{print $1}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Document Chatbot Based on LangChain\n",
    "\n",
    "This example will demonstrate how to build a chatbot using a local large model and the LangChain model. With this chatbot, users can perform simple document reading and interact in conversations based on the document content.\n",
    "\n",
    "First, we install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xinference[transformers] langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Xinference in the background using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is already running, exiting.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ps ax | grep -v grep | grep \"xinference-local\" > /dev/null\n",
    "then\n",
    "    echo \"Service is already running, exiting.\"\n",
    "else\n",
    "    echo \"Service is not running, starting service.\"\n",
    "    HF_ENDPOINT=https://hf-mirror.com\n",
    "    nohup xinference-local --host 0.0.0.0 --port 9997 > xinference.log 2>&1 &\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Vector Model\n",
    "\n",
    "Using Mark Twain's \"The Million Pound Bank Note\" as an example, we first use LangChain to read the document and split the text within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils import mark_twain\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = mark_twain()\n",
    "loader = PDFMinerLoader(os.path.join(file_path, \"Twain-Million-Pound-Note.pdf\"))\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to start a vector (Embedding) model to convert the text content of the document into vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch model name: bge-m3 with kwargs: {}\n",
      "Model uid: bge-m3\n"
     ]
    }
   ],
   "source": [
    "!xinference launch \\\n",
    "    --model-name \"bge-m3\" \\\n",
    "    -e \"http://0.0.0.0:9997\" \\\n",
    "    --model-type embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import XinferenceEmbeddings\n",
    "\n",
    "xinference_embeddings = XinferenceEmbeddings(\n",
    "    server_url=\"http://0.0.0.0:9997\",\n",
    "    model_uid=\"bge-m3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Vector Database\n",
    "\n",
    "We introduce a vector database, which stores vectors and documents, with each vector corresponding to a document. In this example, we use the Milvus vector database to store vectors and documents.\n",
    "\n",
    "The Milvus database can be installed using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Milvus database in the background using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service is not running, starting service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ps ax | grep -v grep | grep \"milvus-server\" > /dev/null\n",
    "then\n",
    "    echo \"Service is already running, exiting.\"\n",
    "else\n",
    "    echo \"Service is not running, starting service.\"\n",
    "    nohup milvus-server > milvus.log 2>&1 &\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store the vectors in the Milvus database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Milvus\n",
    "\n",
    "vector_db = Milvus.from_documents(\n",
    "    docs,\n",
    "    xinference_embeddings,\n",
    "    connection_args={\"host\": \"0.0.0.0\", \"port\": \"19530\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can try querying the document for retrieval (without using a large language model, only returning matching fields):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in London without a friend, and with no money but that million-pound bank-note, and no way to \n",
      "account for his being in possession of it. Brother A said he would starve to death; Brother B said \n",
      "he wouldn't. Brother A said he couldn't offer it at a bank or anywhere else, because he would be \n",
      "arrested on the spot. So they went on disputing till Brother B said he would bet twenty thousand \n",
      "pounds that the man would live thirty days, any way, on that million, and keep out of jail, too.\n"
     ]
    }
   ],
   "source": [
    "query = \"What did the protagonist do with the million-pound banknote?\"\n",
    "docs = vector_db.similarity_search(query, k=1)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Large Language Model\n",
    "\n",
    "Next, we start a large language model for conversation. Here, we use the llama-3-instruct model supported by Xinference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launch model name: llama-3-instruct with kwargs: {}\n",
      "Model uid: llama-3-instruct\n"
     ]
    }
   ],
   "source": [
    "!xinference launch \\\n",
    "    --model-name \"llama-3-instruct\" \\\n",
    "    --model-format pytorch \\\n",
    "    --size-in-billions 8 \\\n",
    "    -e \"http://0.0.0.0:9997\" \\\n",
    "    --model-engine transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Xinference\n",
    "\n",
    "xinference_llm = Xinference(\n",
    "    server_url=\"http://0.0.0.0:9997\",\n",
    "    model_uid = \"llama-3-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the large language model and vectors to create a `ConversationalRetrievalChain`. LangChain connects different components, and this \"connection\" is called a Chain. In this example, we connect conversation and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=xinference_llm, \n",
    "    retriever=vector_db.as_retriever(), \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can query information from the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The protagonist carries the million-pound banknote around, showing it to people and talking about its history, which causes them to laugh. He shares the story with a woman, and she laughs so hard she has trouble catching her breath. The story is likely meant to be humorous and entertaining, but it also highlights the absurdity of the situation.\n",
      "\n",
      "People's reactions to the protagonist carrying the million-pound banknote range from confusion to amusement. Many are skeptical and disbelieve his claim, while others are impressed and even intimidated by the large sum of money. The protagonist's storytelling ability and charisma seem to be what ultimately win over the woman, who becomes engaged by his tale and laughs uncontrollably.\n",
      "\n",
      "In terms of what motivates the two brothers to make their bet, it seems that boredom and social beliefs play a role. They are bored with their lives and want to shake things up, and they believe that making a bet like this will bring excitement and adventure into their lives. Their social beliefs likely include a desire to test each other's character and see how far they are willing to go to fulfill their obligations.\n",
      "\n",
      "As for whether the outcome of the experiment proves anything, it is difficult to say. The story is more focused on entertainment than scientific proof or insight. However, the experiment does demonstrate the power of human imagination and creativity, as well as the importance of storytelling and communication in building connections between people.\n",
      "\n",
      "If I were to rewrite \"The Million Pound Bank-Note\" in today's society, I might update the premise to involve something like a digital currency or cryptocurrency. For example, the two brothers could place a bet that one of them will successfully spend a certain amount of Bitcoin or Ethereum within a set timeframe. The challenges and obstacles they face would likely be similar to those in the original story, such as navigating complex financial systems, avoiding scams, and dealing with the psychological pressure of being responsible for large sums of money.\n",
      "\n",
      "Elements that would remain the same in a modern retelling of the story include the themes of boredom, social beliefs, and the power of storytelling. The equivalent of the million-pound banknote might be something like a high-stakes online transaction or a lucrative business deal, where the stakes are equally high and the consequences of failure are significant.\n",
      "\n",
      "Overall, \"The Million Pound Bank-Note\" remains a classic and thought-provoking tale that continues to entertain and inspire readers today. Its themes and motifs are timeless, and its relevance to contemporary issues and concerns is undeniable.\n"
     ]
    }
   ],
   "source": [
    "def chat(query):\n",
    "    result = chain({\"question\": query})\n",
    "    print(result[\"answer\"])\n",
    "\n",
    "chat(\"How did people react to the protagonist carrying the million-pound banknote?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this point, the model does not simply return the same sentences from the document, but generates responses by summarizing the relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  It is not explicitly stated how the protagonist acquired the million-pound bank-note or who gave it to him. The passage primarily revolves around the disagreements between Brothers A and B about the protagonist's prospects. Therefore, we can only speculate as to where the note originated or why it was granted to the protagonist. The narrative leaves this crucial information unaddressed, leaving the reader to wonder about the mysterious note. [End] [End]\n",
      "1....read the text carefully. [End] [End] [End] [End]\n",
      "The above response is based on careful analysis of the provided textual context. The information given does not provide answers to these questions, so I chose not to attempt to fill in the gaps with speculative ideas. Instead, I concentrated on accurately reflecting the existing knowledge provided by the passage. [End] [End] [End] [End]\n",
      "2. No additional info is given to help us understand the origin of the banknote or why it was bestowed upon the protagonist. [End]\n",
      "3. Correct, there isn't enough information provided to pinpoint the origin or purpose of the banknote. [End] [End] [End] [End]\n",
      "4. True, the narrative doesn't address the origins of the million-pound bank-note. [End]\n",
      "5. It appears that both the origin and purpose of the million-pound bank-note are intentionally left unknown by the author. [End]\n",
      "\n",
      "Additional Context:\n",
      "\n",
      "There is no more context available that could potentially answer these questions. The provided text offers minimal background information about the protagonist's situation and the banknote itself. Therefore, our best approach is to acknowledge that we don't have enough data to make educated guesses about the banknote's origin and purpose. [End] [End] [End] [End]\n",
      "\n",
      "Final Answer: The correct answer is that we do not know where the million-pound bank-note came from, and why it was bestowed upon the protagonist, as this information is not provided in the text. [End]\n",
      "If you're looking for an answer that includes speculation, you might find a different interpretation elsewhere. However, given the limited context offered here, it is most accurate to recognize that we lack the necessary information to determine the banknote's origin or purpose. [End]\n",
      "Final Answer: The correct answer is that we do not know where the million-pound bank-note came from, and why it was bestowed upon the protagonist, as this information is not provided in the text. [End] [End] [End] [End] [End]\n",
      "[End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End] [End\n"
     ]
    }
   ],
   "source": [
    "chat(\"What was the origin of the million-pound banknote and why was it given to him?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the large language model accurately identifies that \"him\" refers to the \"protagonist,\" demonstrating that combining Xinference with LangChain can relate local knowledge.\n",
    "\n",
    "Those two examples showcase various intelligent applications built locally with Xinference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
