{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mpi-point2point)=\n",
    "# Point-to-Point Communication\n",
    "\n",
    "The simplest communication patterns is Point-to-Point communication, which can be further divided into Blocking and Non-Blocking. When implementing Point-to-Point communication, two main considerations are:\n",
    "\n",
    "* How to identify different processes? For example, if you want the process of rank 0 to send a message to the process of\n",
    "rank 1.\n",
    "* What kind of data to send or receive? For example, 1024 integers.\n",
    "\n",
    "## Send and Receive\n",
    "\n",
    "[`Comm.send()`](https://mpi4py.readthedocs.io/en/latest/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.send) and [`Comm.recv()`](https://mpi4py.readthedocs.io/en/latest/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.recv) are used for blocking send and receive, respectively.\n",
    "\n",
    "The key parameters for `Comm.send(obj, dest, tag=0)` are `obj` and `dest`. `obj` is the data we want to send, and it can be a Python built-in data type such as `list` and `dict`, a NumPy `ndarray`, or even CuPy data on a GPU. In {ref}`mpi-hello-world`, we introduced communicator and rank, and you can use the rank number to locate a process. `dest` is the rank number. `tag` provides programmers with more control options. For example, the receiver can choose to only receive messages with specific tags.\n",
    "\n",
    "## Example 1: Send Python Object\n",
    "\n",
    "Here, we show how to send a Python object, which is serialized by [pickle](https://docs.python.org/3/library/pickle.html#module-pickle).\n",
    "\n",
    "```{code-block} python\n",
    ":caption: send-py-object.py\n",
    ":name: mpi-send-py-object\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1)\n",
    "    print(f\"Sended: {data}, from rank: {rank}.\")\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print(f\"Received: {data}, to rank: {rank}.\")\n",
    "```\n",
    "\n",
    "Save the code in a file named `send-py-object.py` and launch it in the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sended: {'a': 7, 'b': 3.14}, from rank: 0.\n",
      "Received: {'a': 7, 'b': 3.14}, to rank: 1.\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 2 python send-py-object.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Send NumPy `ndarray`\n",
    "\n",
    "Send a NumPy `ndarray`:\n",
    "\n",
    "```{code-block} python\n",
    ":caption: send-np.py\n",
    ":name: mpi-send-np\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# tell MPI data type is int\n",
    "# dtype='i', i is short for INT\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype='i')\n",
    "    comm.Send([data, MPI.INT], dest=1)\n",
    "    print(f\"Sended: {data}, from rank: {rank}.\")\n",
    "elif rank == 1:\n",
    "    data = np.empty(10, dtype='i')\n",
    "    comm.Recv([data, MPI.INT], source=0)\n",
    "    print(f\"Received: {data}, to rank: {rank}.\")\n",
    "\n",
    "# MPI detects data type\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype=np.float64)\n",
    "    comm.Send(data, dest=1)\n",
    "    print(f\"Sended: {data}, from rank: {rank}.\")\n",
    "elif rank == 1:\n",
    "    data = np.empty(10, dtype=np.float64)\n",
    "    comm.Recv(data, source=0)\n",
    "    print(f\"Received: {data}, to rank: {rank}.\")\n",
    "```\n",
    "\n",
    "Save as `send-np.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sended: [0 1 2 3 4 5 6 7 8 9], from rank: 0.\n",
      "Received: [0 1 2 3 4 5 6 7 8 9], to rank: 1.\n",
      "Received: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.], to rank: 1.\n",
      "Sended: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.], from rank: 0.\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 2 python send-np.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The initial letters of the `Send` and `Recv` functions are capitalized because these capitalized methods are based on buffers. For these buffer-based functions, it is crucial to explicitly specify the data type, such as passing a binary tuple `[data, MPI.DOUBLE]` or a triple `[data, count, MPI.DOUBLE]`. In {numref}`mpi-send-np`, the `comm.Send(data, dest=1)` does not explicitly inform MPI about the data type and size because MPI automatically detects the type of NumPy and CuPy `ndarray`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Master-Worker\n",
    "\n",
    "In this example, we implement a Master-Worker computation with a total of `size` processes. The first `size-1` processes act as Workers, generating random data. The last process (rank `size-1`) serves as the Master, receiving data and printing its size.\n",
    "\n",
    "The data exchange process between Master and Worker processes is demonstrated in {numref}`mpi-master-worker`.\n",
    "\n",
    "```{code-block} python\n",
    ":caption: master-worker.py\n",
    ":name: mpi-master-worker\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank < size - 1:\n",
    "    # Worker process\n",
    "    np.random.seed(rank)\n",
    "    # Generate random data\n",
    "    data_count = np.random.randint(100)\n",
    "    data = np.random.randint(100, size=data_count)\n",
    "    comm.send(data, dest=size - 1)\n",
    "    print(f\"Worker: worker ID: {rank}; count: {len(data)}\")\n",
    "else:\n",
    "    # Master process\n",
    "    for i in range(size - 1):\n",
    "        status = MPI.Status()\n",
    "        data = comm.recv(source=MPI.ANY_SOURCE, status=status)\n",
    "        print(f\"Master: worker ID: {status.Get_source()}; count: {len(data)}\")\n",
    "\n",
    "comm.Barrier()\n",
    "```\n",
    "\n",
    "In this example, processes with rank less than `size - 1` are Workers, generating random data and sending it to the last process (with rank `size - 1`). The last process receives the data and prints the size of the received data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker: worker ID: 0; count: 44\n",
      "Worker: worker ID: 2; count: 40\n",
      "Worker: worker ID: 4; count: 46\n",
      "Worker: worker ID: 3; count: 24\n",
      "Master: worker ID: 2; count: 40\n",
      "Master: worker ID: 3; count: 24\n",
      "Master: worker ID: 4; count: 46\n",
      "Master: worker ID: 0; count: 44\n",
      "Worker: worker ID: 5; count: 99Master: worker ID: 5; count: 99\n",
      "\n",
      "Worker: worker ID: 1; count: 37\n",
      "Master: worker ID: 1; count: 37\n",
      "Worker: worker ID: 6; count: 10\n",
      "Master: worker ID: 6; count: 10\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 8 python master-worker.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Rectangle Simulation for Calculating $\\pi$\n",
    "\n",
    "For a circle with a radius R, we can use a differential method to divide the circle into N small rectangles. When the number of rectangles, N, approaches infinity, the total area of all rectangles approximates 1/4 of the circle area, as shown in {numref}`rectangle-pi`.\n",
    "\n",
    "```{figure} ../img/ch-mpi/rectangle-pi.svg\n",
    "---\n",
    "width: 500px\n",
    "name: rectangle-pi\n",
    "---\n",
    "Simulating 1/4 of a circle using N small rectangles.\n",
    "```\n",
    "\n",
    "Assuming there are `size` processes involved in the calculation, we first determine the number of rectangles each process needs to handle (`N/size`). Each process calculates the sum of the areas of its rectangles and sends the result to the Master process. The first process acts as the Master, receiving data from each Worker, consolidating all rectangle areas, and thereby approximating the value of $\\pi$.\n",
    "\n",
    "{numref}`mpi-rectangle-pi` shows the process.\n",
    "\n",
    "```{code-block} python\n",
    ":caption: rectangle-pi.py\n",
    ":name: mpi-rectangle-pi\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "communicator = MPI.COMM_WORLD\n",
    "rank = communicator.Get_rank()\n",
    "process_nums = communicator.Get_size()\n",
    "\"\"\"\n",
    "Configuration:\n",
    "R=1\n",
    "N=64*1024*1024\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "rect_num = 64 * 1024 * 1024\n",
    "rect_width = 1 / rect_num\n",
    "step_size = rect_num // process_nums\n",
    "\n",
    "def cal_rect_area(process_no, step_size, rect_width):\n",
    "    total_area = 0.0\n",
    "    rect_start = (process_no * step_size + 1) * rect_width\n",
    "\n",
    "    for i in range(step_size):\n",
    "        x = rect_start + i * rect_width\n",
    "        # (x,y) is the upper right point of the i-th rectangle\n",
    "        # x^2+y^2=1 => y=sqrt(1-x^2)\n",
    "        rect_length = math.pow(1 - x * x, 0.5)\n",
    "        total_area += rect_width * rect_length\n",
    "    return total_area\n",
    "\n",
    "# Calculating on each process\n",
    "total_area = cal_rect_area(rank, step_size, rect_width)\n",
    "\n",
    "if rank == 0:\n",
    "    # Master\n",
    "    for i in range(1, process_nums):\n",
    "        total_area += communicator.recv(source=i)\n",
    "    p_i = total_area * 4\n",
    "    t1 = time.time()\n",
    "    print(\"Simulated PI: {:.10f}, Relative Error：{:.10f}\".format(p_i, abs(1 - p_i / math.pi)))\n",
    "    print(\"Time：{:.3f}s\".format(t1 - t0))\n",
    "else:\n",
    "    # Worker\n",
    "    communicator.send(total_area, dest=0)\n",
    "```\n",
    "\n",
    "In this case, we set the following configurations:`R=1`, `N=64*1024*1024`, and save as `rectangle_pi.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated PI: 3.1415926238, Relative Error：0.0000000095\n",
      "Time：7.361s\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 8 python rectangle_pi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking v.s. Non-blocking\n",
    "\n",
    "### Blocking\n",
    "\n",
    "Let's first analyze blocking communication. The `Send` and `Recv` methods, which are based on buffering:\n",
    "\n",
    "* `Send`: It will not return until the buffer is empty, meaning all the data in the buffer has been sent. The buffer area can then be reused in subsequent `Send`s.\n",
    "\n",
    "* `Recv`: It will not `return` until the buffer is full.\n",
    "\n",
    "As shown in {ref}`mpi-communications`, blocking communication returns only when the data transmission is completed; otherwise, it keeps waiting.\n",
    "\n",
    "```{figure} ../img/ch-mpi/blocking.svg\n",
    "---\n",
    "width: 600px\n",
    "name: blocking-communications\n",
    "---\n",
    "Blocking communications\n",
    "```\n",
    "\n",
    "Code using blocking communication is easier to design, but a common issue is deadlock. For example, in the code below, rank 1 causes a deadlock. The order of `Send` and `Recv` calls should be swapped to avoid this:\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "\tcomm.Send(..to rank 1..)\n",
    "    comm.Recv(..from rank 1..)\n",
    "else if (rank == 1):           <- deadlock\n",
    "    comm.Send(..to rank 0..)   <- should swap Send and Recv\n",
    "    comm.Recv(..from rank 0..)\n",
    "```\n",
    "\n",
    "### Non-blocking\n",
    "\n",
    "In contrast, non-blocking communication does not wait for the completion of data transmission. Non-blocking communication can enhance performance by overlapping communication and computation, i.e., the communications are handled on the network side, meanwhile the computational tasks are performed on the CPU side. The [`isend`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.isend) and [`irecv`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.irecv) methods are used for non-blocking communication:\n",
    "\n",
    "* `isend`: Initiates a non-blocking send operation and immediately returns control to the user, allowing the execution of subsequent code.\n",
    "\n",
    "* `irecv`: Initiates a non-blocking receive operation and immediately returns control to the user, allowing the execution of subsequent code.\n",
    "\n",
    "After a non-blocking communication call, the [`Request`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Request.html#mpi4py.MPI.Request) handle is returned immediately. Subsequently, the programmer can perform further processing on the `Request`, such as waiting for the data transfer associated with the `Request` to complete. Non-blocking communication is denoted by an uppercase 'I' or a lowercase 'i', where 'I' is buffer-based and 'i' is not. \n",
    "The function parameters of [`isend`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.isend) are similar to [`send`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.send), with the key distinction being that `isend` returns a `Request`. The `Request` class provides a `wait()` method, and explicitly calling `wait()` allows for waiting until the data transfer is complete. Code written in a blocking manner using `send` can be modified to utilize non-blocking communication by using `isend` + `Request.wait()`.\n",
    "\n",
    "Non-blocking communication is illustrated in {ref}`mpi-non-blocking`.\n",
    "\n",
    "```{code-block} python\n",
    ":caption: non-blocking.py\n",
    ":name: mpi-non-blocking\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    req = comm.isend(data, dest=1, tag=11)\n",
    "    print(f\"Sending: {data}, from rank: {rank}.\")\n",
    "    req.wait()\n",
    "    print(f\"Sended: {data}, from rank: {rank}.\")\n",
    "elif rank == 1:\n",
    "    req = comm.irecv(source=0, tag=11)\n",
    "    print(f\"Receiving: to rank: {rank}.\")\n",
    "    data = req.wait()\n",
    "    print(f\"Received: {data}, to rank: {rank}.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiving: to rank: 1.\n",
      "Sending: {'a': 7, 'b': 3.14}, from rank: 0.\n",
      "Sended: {'a': 7, 'b': 3.14}, from rank: 0.\n",
      "Received: {'a': 7, 'b': 3.14}, to rank: 1.\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 8 python non-blocking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{numref}`non-blocking-communications` demonstrates the data flow changes of non-blocking communication.\n",
    "\n",
    "```{figure} ../img/ch-mpi/non-blocking.svg\n",
    "---\n",
    "width: 600px\n",
    "name: non-blocking-communications\n",
    "---\n",
    "Non-blocking communications\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
